{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fadfb54-4eaa-4cca-a77a-b6852bfe16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pathlib\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from xai_sdk import Client\n",
    "from xai_sdk.chat import user\n",
    "from xai_sdk.search import SearchParameters, web_source, news_source, x_source, rss_source\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5743a16-c41c-4f80-ba05-056faa6d4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your schema\n",
    "REQUIRED_FIELDS = [\"title\", \"description\", \"state\", \"lga\", \"status\"]\n",
    "OPTIONAL_FIELDS_i = [\"incidentDate\", \"incidentTime\"]\n",
    "OPTIONAL_FIELDS_ii = [\"lat\", \"lng\"]\n",
    "ALL_FIELDS = REQUIRED_FIELDS + OPTIONAL_FIELDS_i + OPTIONAL_FIELDS_ii + [\"is_duplicate\"]\n",
    "\n",
    "\n",
    "def normalize_news(news_data):\n",
    "    \"\"\"\n",
    "    Normalize list of dicts into a DataFrame with consistent schema.\n",
    "    Missing fields are filled with None.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(news_data)\n",
    "    for col in ALL_FIELDS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None  # fill missing fields\n",
    "    # Ensure correct column order\n",
    "    df = df[ALL_FIELDS]\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_news_to_csv(news_data, folder=\"news_data\"):\n",
    "    \"\"\"Save current run news into a timestamped CSV\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    curr_dt = datetime.now()\n",
    "    timestamp = curr_dt.strftime(\"%Y%m%d_%H\")\n",
    "    exec_month = curr_dt.strftime(\"%Y-%m\")\n",
    "    filepath = os.path.join(folder, exec_month, f\"{timestamp}.csv\")\n",
    "    df = normalize_news(news_data)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"‚úÖ Saved {len(df)} news to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_recent_news(folder=\"news_data\", days=5):\n",
    "    \"\"\"Load news from the last N days of CSVs\"\"\"\n",
    "    curr_dt = datetime.now()\n",
    "    exec_month = curr_dt.strftime(\"%Y-%m\")\n",
    "    \n",
    "    cutoff = datetime.now() - timedelta(days=days)\n",
    "    dfs = []\n",
    "    for fname in os.listdir(os.path.join(folder, exec_month)):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            try:\n",
    "                file_date = datetime.strptime(fname.split(\".\")[0], \"%Y%m%d_%H\")\n",
    "                if file_date >= cutoff:\n",
    "                    dfs.append(pd.read_csv(os.path.join(folder, exec_month, fname)))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "def deduplicate_news(current_news, past_news, threshold=0.35):\n",
    "    \"\"\"Remove duplicates by cosine similarity on title+description\"\"\"\n",
    "    if past_news.empty:\n",
    "        current_news[\"is_duplicate\"] = False\n",
    "        return current_news\n",
    "\n",
    "    combined_past = (\n",
    "        past_news[\"title\"].fillna(\"\") + \" \" + past_news[\"description\"].fillna(\"\")\n",
    "    ).tolist()\n",
    "    combined_current = (\n",
    "        current_news[\"title\"].fillna(\"\") + \" \" + current_news[\"description\"].fillna(\"\")\n",
    "    ).tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit(combined_past + combined_current)\n",
    "    past_vecs = vectorizer.transform(combined_past)\n",
    "    curr_vecs = vectorizer.transform(combined_current)\n",
    "\n",
    "    duplicates = []\n",
    "    for i, vec in enumerate(curr_vecs):\n",
    "        sim = cosine_similarity(vec, past_vecs).max()\n",
    "        print(sim)\n",
    "        duplicates.append(sim >= threshold)\n",
    "\n",
    "    current_news[\"is_duplicate\"] = duplicates\n",
    "    return current_news\n",
    "\n",
    "def publish_news(api_key: str, news_items: list):\n",
    "    \"\"\"\n",
    "    Publishes news items to the Convex threats API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your Convex API key (string starting with stmp_...).\n",
    "        news_items (list): A list of dicts containing threat data. \n",
    "                           Must include required fields:\n",
    "                           title, description, state, lga, status\n",
    "                           Optional fields: incidentDate, incidentTime, lat, lng\n",
    "    \"\"\"\n",
    "    url = f\"https://fantastic-mammoth-699.convex.site/api/threats?api_key={api_key}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=news_items)\n",
    "        response.raise_for_status()\n",
    "        print(\"‚úÖ Successfully published news!\")\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"‚ùå HTTP error occurred: {http_err} - {response.text}\")\n",
    "    except Exception as err:\n",
    "        print(f\"‚ùå Other error occurred: {err}\")\n",
    "\n",
    "def fetch_security_news(api_key: str):\n",
    "    \"\"\"\n",
    "    Fetch security-related news in Nigeria from the last 8 hours.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Your XAI API key.\n",
    "\n",
    "    Returns:\n",
    "        list | dict: Parsed JSON object with news items if successful, \n",
    "                     or None if parsing fails.\n",
    "    \"\"\"\n",
    "    client = Client(api_key=api_key)\n",
    "\n",
    "    # Dynamic dates for last 8 hours\n",
    "    from_date = datetime.now() - timedelta(hours=8)\n",
    "    to_date = datetime.now()\n",
    "\n",
    "    search_config = SearchParameters(\n",
    "        mode=\"on\",\n",
    "        return_citations=True,\n",
    "        from_date=from_date,\n",
    "        to_date=to_date,\n",
    "        max_search_results=30,\n",
    "        sources=[\n",
    "            web_source(country=\"NG\"),\n",
    "            news_source(country=\"NG\"),\n",
    "            x_source(),\n",
    "            rss_source(links=[\n",
    "                'https://news.google.com/rss/search?q=nigeria+security&hl=en-NG&gl=NG&ceid=NG:en'\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chat = client.chat.create(\n",
    "        model=\"grok-4-fast-reasoning-latest\",\n",
    "        messages=[user(\n",
    "            \"\"\"I need news from the last 8 hours related to security issues such as banditry, gunmen or any kind on violent activities\n",
    "            in Nigeria. I need the output json formatted. For each news, return like this:\n",
    "            {\"title\": \"<title>\", \n",
    "             \"description\":\"<summary>\", \n",
    "             \"state\":\"<state of occurrence>\", \n",
    "             \"lga\":\"<Please try to infer the lga from the news, the lga is compulsory>\", \n",
    "             \"incidentDate\":\"<date>\", \n",
    "             \"incidentTime\":\"<time if available>\",\n",
    "             \"status\":\"High\" or \"Medium\" or \"Low\"}.\n",
    "            If any of the fields isn't available return Null, but for date or time, \n",
    "            return current date and for time return 00:00.\"\"\"\n",
    "        )],\n",
    "        search_parameters=search_config\n",
    "    )\n",
    "\n",
    "    response = chat.sample()\n",
    "\n",
    "    try:\n",
    "        news_data = json.loads(response.content)  # Assumes valid JSON array/object\n",
    "        return news_data\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ö†Ô∏è Content is not valid JSON‚Äîcheck the raw output instead.\")\n",
    "        return None\n",
    "\n",
    "def quick_replace(val):\n",
    "    if val==None:\n",
    "        return \"Somewhere\"\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "def filter_and_publish(news_data, api_key, folder=\"news_data\"):\n",
    "    \"\"\"Workflow to save, deduplicate, and publish only unique news\"\"\"\n",
    "\n",
    "    # Load past 5 days\n",
    "    past_news = load_recent_news(folder, days=5)\n",
    "    display(past_news)\n",
    "\n",
    "    # Save current\n",
    "    filepath = save_news_to_csv(news_data, folder)\n",
    "\n",
    "    # Deduplicate\n",
    "    current_df = normalize_news(news_data)\n",
    "    deduped = deduplicate_news(current_df, past_news)\n",
    "\n",
    "    # Filter unique\n",
    "    unique_news = deduped[deduped[\"is_duplicate\"] == False]\n",
    "    unique_news['lga'] = unique_news['lga'].apply(quick_replace)\n",
    "    unique_news=unique_news[REQUIRED_FIELDS + OPTIONAL_FIELDS_i]\n",
    "\n",
    "    print(f\"üìä Found {len(unique_news)} unique news out of {len(current_df)}\")\n",
    "\n",
    "    if not unique_news.empty:\n",
    "        print(unique_news.to_dict(orient=\"records\"))\n",
    "        publish_news(api_key, unique_news.to_dict(orient=\"records\"))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No unique news to publish.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b427464-d740-4861-b0d6-1f9559f7a536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>state</th>\n",
       "      <th>lga</th>\n",
       "      <th>status</th>\n",
       "      <th>incidentDate</th>\n",
       "      <th>incidentTime</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oke-Ode: Tragedy as Bandits Kill 15 Vigilantes...</td>\n",
       "      <td>Bandits invaded the Oke-Ode community and kill...</td>\n",
       "      <td>Kwara</td>\n",
       "      <td>Ifelodun</td>\n",
       "      <td>High</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAKING: Bandits Abduct Businessman In Midnig...</td>\n",
       "      <td>Bandits carried out a midnight attack on a com...</td>\n",
       "      <td>Kwara</td>\n",
       "      <td>Ifelodun</td>\n",
       "      <td>High</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSCDC begins manhunt for attackers of Jigawa p...</td>\n",
       "      <td>Attackers targeted students at a polytechnic i...</td>\n",
       "      <td>Jigawa</td>\n",
       "      <td>Dutse</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Oke-Ode: Tragedy as Bandits Kill 15 Vigilantes...   \n",
       "1  BREAKING: Bandits Abduct Businessman In Midnig...   \n",
       "2  NSCDC begins manhunt for attackers of Jigawa p...   \n",
       "\n",
       "                                         description   state       lga  \\\n",
       "0  Bandits invaded the Oke-Ode community and kill...   Kwara  Ifelodun   \n",
       "1  Bandits carried out a midnight attack on a com...   Kwara  Ifelodun   \n",
       "2  Attackers targeted students at a polytechnic i...  Jigawa     Dutse   \n",
       "\n",
       "   status incidentDate incidentTime  lat  lng  is_duplicate  \n",
       "0    High   2025-09-28        00:00  NaN  NaN           NaN  \n",
       "1    High   2025-09-28        00:00  NaN  NaN           NaN  \n",
       "2  Medium   2025-09-28        00:00  NaN  NaN           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 4 news to news_data\\2025-09\\20250928_18.csv\n",
      "0.8837435627056051\n",
      "0.3620364225508219\n",
      "0.757058054769306\n",
      "0.18522228700484397\n",
      "üìä Found 2 unique news out of 4\n",
      "[{'title': 'UPDATE: Bandits Kill 10, Abduct Several in Oke Ode Community Despite Govt‚Äôs Denial', 'description': 'Bandits invaded Oke-Ode community, killing 10 and abducting several residents, contrary to government claims of repelling the attack.', 'state': 'Kwara', 'lga': 'Ifelodun', 'status': 'High', 'incidentDate': '2025-09-28', 'incidentTime': '00:00'}, {'title': 'Copyright Commission condemns attack on personnel, NSCDC officers in Sagamu', 'description': 'Attack on Nigerian Copyright Commission personnel and NSCDC officers in Sagamu, Ogun State, prompting condemnation and calls for action.', 'state': 'Ogun', 'lga': 'Sagamu', 'status': 'Medium', 'incidentDate': '2025-09-28', 'incidentTime': '00:00'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Udoh\\AppData\\Local\\Temp\\ipykernel_14860\\3920971965.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_news['lga'] = unique_news['lga'].apply(quick_replace)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully published news!\n"
     ]
    }
   ],
   "source": [
    "# Example integration after you parse your API response\n",
    "try:\n",
    "    with open(\"Grok/key.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    grok_api_key = config[\"grok_api_key\"]\n",
    "    convex_api_key = config[\"convex_api_key\"]\n",
    "    \n",
    "    news_data = fetch_security_news(grok_api_key)\n",
    "    \n",
    "    if isinstance(news_data, list) and news_data:\n",
    "        filter_and_publish(news_data, api_key=convex_api_key)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No news items returned.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ùå Could not parse response content as JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567c1d9-c32e-4fa2-b08e-46937996136f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
