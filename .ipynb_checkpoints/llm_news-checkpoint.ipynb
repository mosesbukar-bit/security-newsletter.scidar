{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fadfb54-4eaa-4cca-a77a-b6852bfe16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pathlib\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from xai_sdk import Client\n",
    "from xai_sdk.chat import user\n",
    "from xai_sdk.search import SearchParameters, web_source, news_source, x_source, rss_source\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6da0480-1948-4b97-b773-85c54acaaaa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Response \n",
      " [\n",
      "  {\n",
      "    \"title\": \"Bandits Abduct Businessman In Midnight Attack On Kwara Community\",\n",
      "    \"description\": \"Armed bandits carried out a midnight raid on a community in Kwara State, abducting a prominent businessman. The attack highlights ongoing security challenges in the region, as reported by Sahara Reporters.\",\n",
      "    \"state\": \"Kwara\",\n",
      "    \"lga\": null,\n",
      "    \"incident date\": \"2025-09-28\",\n",
      "    \"incidentTime\": \"midnight\"\n",
      "  }\n",
      "]\n",
      "\n",
      " \n",
      " Citations \n",
      " ['https://saharareporters.com/2025/09/28/breaking-bandits-abduct-businessman-midnight-attack-kwara-community', 'https://modernghana.com/news/1432967/nigeria-fights-back-against-banditry-and-insecurit.amp', 'https://thewillnews.com/opinion-nigeria-fights-back-against-banditry-and-insecurity', 'https://defenceweb.co.za/security/civil-security/armed-banditry-is-becoming-a-crisis-in-nigeria-why-fixing-the-police-is-key', 'https://link.springer.com/article/10.1057/s41284-025-00477-1', 'https://www.tandfonline.com/doi/full/10.1080/17419166.2023.2164924', 'https://guardian.ng/opinion/as-nigeria-fights-back-against-banditry-kidnapping', 'https://www.tandfonline.com/doi/full/10.1080/17467586.2024.2356509', 'https://crisisgroup.org/africa/west-africa/nigeria', 'https://www.americansecurityproject.org/combating-banditry-in-northwest-nigeria/', 'https://thesoufancenter.org/intelbrief-2025-august-22/']\n"
     ]
    }
   ],
   "source": [
    "# Trying Grok - Initial Code\n",
    "client = Client(api_key=\"\")\n",
    "\n",
    "search_config = SearchParameters(\n",
    "        mode=\"on\",\n",
    "        return_citations=True,\n",
    "        from_date=datetime(2025, 9, 28),\n",
    "        to_date=datetime(2025, 9, 28),\n",
    "        max_search_results=30,\n",
    "        sources=[web_source(country=\"NG\"), news_source(country=\"NG\"), x_source(), rss_source()]\n",
    "    )\n",
    "        \n",
    "chat = client.chat.create(\n",
    "    model=\"grok-4-fast-reasoning-latest\",\n",
    "    messages=[user(\"\"\"I need news from the last 8 hours related to security issues such as banditry, gunmen or any kind on violent activities\n",
    "    in Nigeria. I need the output json formated. For each news, return like this {\"title\": \"<let here be a befitting title>\", \n",
    "    \"description\":\"<Summary of the incident from the news source>\", \"state\":\"<the state of occurence>\", \"lga\":\"<local government of occurence \n",
    "    if mentioned in the news>\", \"incident date\":\"<todays date or whatever date of incident>\", \"incidentTime\":\"<incident time if available>\"}. \n",
    "    If any of the fields isn't available return Null but for date or time, return current date and current time.\"\"\")],\n",
    "    search_parameters=search_config\n",
    ")\n",
    "\n",
    "response = chat.sample()\n",
    "print(\"Full Response \\n\", response.content)\n",
    "\n",
    "print(\"\\n \\n Citations \\n\", response.citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1825553-47f6-4099-adee-d977f7fd7362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2313309b-fb9a-49a0-824a-24a62439c4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Response Content:\n",
      " [\n",
      "  {\n",
      "    \"title\": \"Bandits Abduct Businessman in Midnight Attack on Kwara Community\",\n",
      "    \"description\": \"Armed bandits launched a midnight raid on a community in Kwara State, abducting a prominent businessman during the attack.\",\n",
      "    \"state\": \"Kwara\",\n",
      "    \"lga\": null,\n",
      "    \"incident date\": \"2025-09-28\",\n",
      "    \"incidentTime\": \"00:00\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Bandits Kill 10 Vigilantes and Abduct Several in Oke Ode Community, Kwara\",\n",
      "    \"description\": \"Gunmen suspected to be bandits attacked Oke Ode community in Kwara State, killing 10 vigilantes and abducting several residents, despite government claims of repelling the assault.\",\n",
      "    \"state\": \"Kwara\",\n",
      "    \"lga\": \"Ifelodun\",\n",
      "    \"incident date\": \"2025-09-28\",\n",
      "    \"incidentTime\": null\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"NSCDC Launches Manhunt for Attackers of Jigawa Polytechnic Students\",\n",
      "    \"description\": \"The Nigeria Security and Civil Defence Corps has initiated a search for assailants who attacked students at a polytechnic in Jigawa State, highlighting ongoing security threats in educational institutions.\",\n",
      "    \"state\": \"Jigawa\",\n",
      "    \"lga\": null,\n",
      "    \"incident date\": \"2025-09-28\",\n",
      "    \"incidentTime\": null\n",
      "  }\n",
      "]\n",
      "\n",
      "Citations:\n",
      " ['https://saharareporters.com/2025/09/28/breaking-bandits-abduct-businessman-midnight-attack-kwara-community', 'https://modernghana.com/news/1432967/nigeria-fights-back-against-banditry-and-insecurit.amp', 'https://defenceweb.co.za/security/civil-security/armed-banditry-is-becoming-a-crisis-in-nigeria-why-fixing-the-police-is-key', 'https://link.springer.com/article/10.1057/s41284-025-00477-1', 'https://www.tandfonline.com/doi/full/10.1080/17419166.2023.2164924', 'https://guardian.ng/opinion/as-nigeria-fights-back-against-banditry-kidnapping', 'https://www.tandfonline.com/doi/full/10.1080/17467586.2024.2356509', 'https://crisisgroup.org/africa/west-africa/nigeria', 'https://www.americansecurityproject.org/combating-banditry-in-northwest-nigeria/', 'https://x.com/jayjayehi/status/1972230396755771875', 'https://x.com/AsuitTutors/status/1972270763949359487', 'https://x.com/BELVAZ89/status/1972264739443995018', 'https://x.com/kingdezest/status/1972307614752256205', 'https://x.com/justeventsonlin/status/1972276304486535242', 'https://x.com/owoniyilb/status/1972264058675224946', 'https://news.google.com/rss/articles/CBMihgFBVV95cUxNVXM0WjVTeGJoVnFpakswbUg3Z1ZrS2wxaWlqcS1Bc3pSeGFWeW5VZU9ReHFPOEVCUmEzTS0zUV9RTUkzeHlxejZzeWxURzQtOE1lanV6SXEyZFBwaE5ZbEt0c1VhSmRPT2lqMXJ0ZjBlWFlydmxFRFVZSjdpYWdhVVVEVUNkUQ?oc=5', 'https://news.google.com/rss/articles/CBMimgFBVV95cUxOMXN1NkRrU3d5bXNRVzBDUFFIbGU4bnVjNUl2ejc5RXZfcHpZVjJzVzJrU2htOG1hUElOcDJKU25SNFNnVHNXQ2ZrR1hqTkFQV3VrSHVrVk1mU1BYWC12MzRGWEJyUVZ0S2g4LWJqcllKbjlaUHF2azhRUENubHFJVG40ZnU4bmJTWHNvejc0V2lVVHRnTHJSSXRB?oc=5', 'https://news.google.com/rss/articles/CBMi2gFBVV95cUxQbTBYblhSS3hNNmExblQ3bEZaeU13YVlTQlEtTW1hdnN2Rlc1cWJFX0tMNnRQSk54UjFCUHBPUXhmeWFtUm1yTlp1d0F0SFNkMmhMTk9FdnlYY3h2dTBnWHBwLWFvTHZfdkVRYW1hVkpwMlN0Mm5oYTJ6b2RWN2Q2cWt3SzRlWFplTzI4RjFGWm1lWmZVMkdFWnZTTWhDeHYxaUcwcEo4cXhJcW5ocmVxdDJHMTN3SnRnemlXSUZuTmg1WnVqWjhkT0pTeGJCOUVlbE5Cek9pMnlQZw?oc=5', 'https://news.google.com/rss/articles/CBMi2AFBVV95cUxOa2FSUXRWWU5vamo2Z2xEdVZhc0VKOEVCZDRNdEFxczNrMkhOQkZ5V2U0MXpMbDlZQUhRZEtVRUx1VlZKUWtDb3ZlVS16dHdqRzlnaG9XcDM1UTdtUXNoSGRhYjc0S0c2VFFNZGNWc055MEVwYzlCZVpDTmp1NDJQQ3VmcklIYkVTSFVfYkJ6cDkwYVlZdXNmdE5pZGRZVTV5Z2NBZHU2MThZejh2NnM3Z1kyRGVHaG9SNXJnem5qRGVJZ3MtVW8zSy12bG9TeEpiM0VYZEVlVEw?oc=5', 'https://news.google.com/rss/articles/CBMirAFBVV95cUxOU2lTN3M4elFzZjQ4amxVQldEQWdhcGF3ejg3aHc1S3ByMU9NQlV3RUhyc2hudTNFRkxqd0t3RmUyTW04cktuTTBuTzZCX3d4cEwtRmxWckwxaGdIY21SN2tBeE1JMk0wYUFsSUtZcVZWMDhLUzhyTEJtaXQtVGgtTlBIbWdnRHp6QlYzeVktVl9iZ2hEbHozdU1rcUdPajA1UzZNdzdWaC1maXRN0gGyAUFVX3lxTE9maEdWcHhsRGk3cU9XVGl0Q2pPakRCS01QUkZqZERaSTBFNGtyZWRFSVV6R0RqekNBWTBHTFYxb1I0ZGptaGpwR3VBcE56eFNFTGVtZ0VzbEprTkduaWpDZ0J2NTBKcWp0Tlhnd0N4cHQxZVNoOEFScEVWV3R4RlpHb2JzalhIb1Q4Qk1ldWRKMWxKZEM0Z0tnblRvUnhMbWFjbDNQYjdPbzU3d2FLQkE1R2c?oc=5', 'https://news.google.com/rss/articles/CBMiogFBVV95cUxQN0hSZWlmd0dBM0VSQUt3QzdQaGh3bmZuZUw0MEtYbVI4ZnJmemlGcHNTRDVMdXNSNENnUFhnMXhDM2QxUnJHZkZuSWhfVG5BNkVlSlVYS2JwbnFYN296QUpFaXJXSGpHQlBma0ZIZ0tXTHFZeE1wb3E1V2U1NUM1Ti1udS11RF9YSlVfLVM1cFBpMjBtSTNwN0EwNURoZ1FQMFE?oc=5', 'https://news.google.com/rss/articles/CBMioAFBVV95cUxQTllHeVpDNXQ0LTVTRm9jdmdTZFNuN21oY3hhaEJacEJIM1pwd2FZSnpoN0FkN1lIUTdZZXpTNmNhcUowVVZlalBuaEJESHpHdVZteERKY1FrTk9HNHMwRnZoTUhjUkZZYW8tUjhYQ2VnRDYzR2ZqU0lkN0NhaDUwcS1ZYU9ONnhyWVJfdlVhTVpVSTJfRUJ1ZHVFblNoQzZ5?oc=5']\n",
      "\n",
      "Parsed News Items:\n",
      " [\n",
      "  {\n",
      "    \"title\": \"Bandits Abduct Businessman in Midnight Attack on Kwara Community\",\n",
      "    \"description\": \"Armed bandits launched a midnight raid on a community in Kwara State, abducting a prominent businessman during the attack.\",\n",
      "    \"state\": \"Kwara\",\n",
      "    \"lga\": null,\n",
      "    \"incident date\": \"2025-09-28\",\n",
      "    \"incidentTime\": \"00:00\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Bandits Kill 10 Vigilantes and Abduct Several in Oke Ode Community, Kwara\",\n",
      "    \"description\": \"Gunmen suspected to be bandits attacked Oke Ode community in Kwara State, killing 10 vigilantes and abducting several residents, despite government claims of repelling the assault.\",\n",
      "    \"state\": \"Kwara\",\n",
      "    \"lga\": \"Ifelodun\",\n",
      "    \"incident date\": \"2025-09-28\",\n",
      "    \"incidentTime\": null\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"NSCDC Launches Manhunt for Attackers of Jigawa Polytechnic Students\",\n",
      "    \"description\": \"The Nigeria Security and Civil Defence Corps has initiated a search for assailants who attacked students at a polytechnic in Jigawa State, highlighting ongoing security threats in educational institutions.\",\n",
      "    \"state\": \"Jigawa\",\n",
      "    \"lga\": null,\n",
      "    \"incident date\": \"2025-09-28\",\n",
      "    \"incidentTime\": null\n",
      "  }\n",
      "]\n",
      "\n",
      "Full Response Object Attributes:\n",
      " ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_choice', '_index', '_proto', 'citations', 'content', 'finish_reason', 'id', 'logprobs', 'process_chunk', 'proto', 'reasoning_content', 'request_settings', 'role', 'system_fingerprint', 'tool_calls', 'usage']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json  # Add this for parsing\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from xai_sdk import Client\n",
    "from xai_sdk.chat import user\n",
    "from xai_sdk.search import SearchParameters, web_source, news_source, x_source, rss_source\n",
    "\n",
    "client = Client(api_key=\"...\")  # Replace with your real key\n",
    "\n",
    "# Dynamic dates for last 8 hours\n",
    "from_date = datetime.now() - timedelta(hours=8)\n",
    "to_date = datetime.now()\n",
    "\n",
    "search_config = SearchParameters(\n",
    "    mode=\"on\",\n",
    "    return_citations=True,\n",
    "    from_date=from_date,\n",
    "    to_date=to_date,\n",
    "    max_search_results=30,\n",
    "    sources=[web_source(country=\"NG\"), news_source(country=\"NG\"), x_source(), rss_source(links=['https://news.google.com/rss/search?q=nigeria+security&hl=en-NG&gl=NG&ceid=NG:en'])]\n",
    ")\n",
    "    \n",
    "chat = client.chat.create(\n",
    "    model=\"grok-4-fast-reasoning-latest\",\n",
    "    messages=[user(\"\"\"I need news from the last 8 hours related to security issues such as banditry, gunmen or any kind on violent activities\n",
    "    in Nigeria. I need the output json formated. For each news, return like this {\"title\": \"<let here be a befitting title>\", \n",
    "    \"description\":\"<Summary of the incident from the news source>\", \"state\":\"<the state of occurence>\", \"lga\":\"<local government of occurence \n",
    "    if mentioned in the news>\", \"incidentDate\":\"<todays date or whatever date of incident>\", \"incidentTime\":\"<incident time if available>\",\n",
    "    \"status\":\"High\" or \"Medium\" or \"Low\" depending on the severity of the news}. \n",
    "    If any of the fields isn't available return Null but for date or time, return current date and current time.\"\"\")],\n",
    "    search_parameters=search_config\n",
    ")\n",
    "\n",
    "response = chat.sample()\n",
    "#print(\"Full Response Content:\\n\", response.content)\n",
    "\n",
    "#print(\"\\nCitations:\\n\", response.citations)\n",
    "\n",
    "# Parse the JSON news items if present in content\n",
    "try:\n",
    "    news_data = json.loads(response.content)  # Assumes content is a JSON array/object\n",
    "    print(\"\\nParsed News Items:\\n\", json.dumps(news_data, indent=2))\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\nContent is not valid JSON—check the raw output above.\")\n",
    "\n",
    "# Debug: See everything in response\n",
    "print(\"\\nFull Response Object Attributes:\\n\", dir(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5743a16-c41c-4f80-ba05-056faa6d4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your schema\n",
    "REQUIRED_FIELDS = [\"title\", \"description\", \"state\", \"lga\", \"status\"]\n",
    "OPTIONAL_FIELDS = [\"incidentDate\", \"incidentTime\", \"lat\", \"lng\"]\n",
    "ALL_FIELDS = REQUIRED_FIELDS + OPTIONAL_FIELDS + [\"is_duplicate\"]\n",
    "\n",
    "\n",
    "def normalize_news(news_data):\n",
    "    \"\"\"\n",
    "    Normalize list of dicts into a DataFrame with consistent schema.\n",
    "    Missing fields are filled with None.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(news_data)\n",
    "    for col in ALL_FIELDS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None  # fill missing fields\n",
    "    # Ensure correct column order\n",
    "    df = df[ALL_FIELDS]\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_news_to_csv(news_data, folder=\"news_data\"):\n",
    "    \"\"\"Save current run news into a timestamped CSV\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    curr_dt = datetime.now()\n",
    "    timestamp = curr_dt.strftime(\"%Y%m%d_%H\")\n",
    "    exec_month = curr_dt.strftime(\"%Y-%m\")\n",
    "    filepath = os.path.join(folder, exec_month, f\"{timestamp}.csv\")\n",
    "    df = normalize_news(news_data)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"✅ Saved {len(df)} news to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_recent_news(folder=\"news_data\", days=5):\n",
    "    \"\"\"Load news from the last N days of CSVs\"\"\"\n",
    "    curr_dt = datetime.now()\n",
    "    exec_month = curr_dt.strftime(\"%Y-%m\")\n",
    "    \n",
    "    cutoff = datetime.now() - timedelta(days=days)\n",
    "    dfs = []\n",
    "    for fname in os.listdir(os.path.join(folder, exec_month)):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            try:\n",
    "                file_date = datetime.strptime(fname.split(\".\")[0], \"%Y%m%d_%H\")\n",
    "                if file_date >= cutoff:\n",
    "                    dfs.append(pd.read_csv(os.path.join(folder, exec_month, fname)))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "def deduplicate_news(current_news, past_news, threshold=0.85):\n",
    "    \"\"\"Remove duplicates by cosine similarity on title+description\"\"\"\n",
    "    if past_news.empty:\n",
    "        current_news[\"is_duplicate\"] = False\n",
    "        return current_news\n",
    "\n",
    "    combined_past = (\n",
    "        past_news[\"title\"].fillna(\"\") + \" \" + past_news[\"description\"].fillna(\"\")\n",
    "    ).tolist()\n",
    "    combined_current = (\n",
    "        current_news[\"title\"].fillna(\"\") + \" \" + current_news[\"description\"].fillna(\"\")\n",
    "    ).tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit(combined_past + combined_current)\n",
    "    past_vecs = vectorizer.transform(combined_past)\n",
    "    curr_vecs = vectorizer.transform(combined_current)\n",
    "\n",
    "    duplicates = []\n",
    "    for i, vec in enumerate(curr_vecs):\n",
    "        sim = cosine_similarity(vec, past_vecs).max()\n",
    "        duplicates.append(sim >= threshold)\n",
    "\n",
    "    current_news[\"is_duplicate\"] = duplicates\n",
    "    return current_news\n",
    "\n",
    "def publish_news(api_key: str, news_items: list):\n",
    "    \"\"\"\n",
    "    Publishes news items to the Convex threats API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your Convex API key (string starting with stmp_...).\n",
    "        news_items (list): A list of dicts containing threat data. \n",
    "                           Must include required fields:\n",
    "                           title, description, state, lga, status\n",
    "                           Optional fields: incidentDate, incidentTime, lat, lng\n",
    "    \"\"\"\n",
    "    url = f\"https://fantastic-mammoth-699.convex.site/api/threats?api_key={api_key}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=news_items)\n",
    "        response.raise_for_status()\n",
    "        print(\"✅ Successfully published news!\")\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"❌ HTTP error occurred: {http_err} - {response.text}\")\n",
    "    except Exception as err:\n",
    "        print(f\"❌ Other error occurred: {err}\")\n",
    "\n",
    "def fetch_security_news(api_key: str):\n",
    "    \"\"\"\n",
    "    Fetch security-related news in Nigeria from the last 8 hours.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Your XAI API key.\n",
    "\n",
    "    Returns:\n",
    "        list | dict: Parsed JSON object with news items if successful, \n",
    "                     or None if parsing fails.\n",
    "    \"\"\"\n",
    "    client = Client(api_key=api_key)\n",
    "\n",
    "    # Dynamic dates for last 8 hours\n",
    "    from_date = datetime.now() - timedelta(hours=8)\n",
    "    to_date = datetime.now()\n",
    "\n",
    "    search_config = SearchParameters(\n",
    "        mode=\"on\",\n",
    "        return_citations=True,\n",
    "        from_date=from_date,\n",
    "        to_date=to_date,\n",
    "        max_search_results=30,\n",
    "        sources=[\n",
    "            web_source(country=\"NG\"),\n",
    "            news_source(country=\"NG\"),\n",
    "            x_source(),\n",
    "            rss_source(links=[\n",
    "                'https://news.google.com/rss/search?q=nigeria+security&hl=en-NG&gl=NG&ceid=NG:en'\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chat = client.chat.create(\n",
    "        model=\"grok-4-fast-reasoning-latest\",\n",
    "        messages=[user(\n",
    "            \"\"\"I need news from the last 8 hours related to security issues such as banditry, gunmen or any kind on violent activities\n",
    "            in Nigeria. I need the output json formatted. For each news, return like this:\n",
    "            {\"title\": \"<title>\", \n",
    "             \"description\":\"<summary>\", \n",
    "             \"state\":\"<state of occurrence>\", \n",
    "             \"lga\":\"<local government if mentioned>\", \n",
    "             \"incidentDate\":\"<date>\", \n",
    "             \"incidentTime\":\"<time if available>\",\n",
    "             \"status\":\"High\" or \"Medium\" or \"Low\"}.\n",
    "            If any of the fields isn't available return Null, but for date or time, \n",
    "            return current date and current time.\"\"\"\n",
    "        )],\n",
    "        search_parameters=search_config\n",
    "    )\n",
    "\n",
    "    response = chat.sample()\n",
    "\n",
    "    try:\n",
    "        news_data = json.loads(response.content)  # Assumes valid JSON array/object\n",
    "        return news_data\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"⚠️ Content is not valid JSON—check the raw output instead.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def filter_and_publish(news_data, api_key, folder=\"news_data\"):\n",
    "    \"\"\"Workflow to save, deduplicate, and publish only unique news\"\"\"\n",
    "    # Save current\n",
    "    filepath = save_news_to_csv(news_data, folder)\n",
    "\n",
    "    # Load past 5 days\n",
    "    past_news = load_recent_news(folder, days=5)\n",
    "\n",
    "    # Deduplicate\n",
    "    current_df = normalize_news(news_data)\n",
    "    deduped = deduplicate_news(current_df, past_news)\n",
    "\n",
    "    # Filter unique\n",
    "    unique_news = deduped[deduped[\"is_duplicate\"] == False]\n",
    "\n",
    "    print(f\"📊 Found {len(unique_news)} unique news out of {len(current_df)}\")\n",
    "\n",
    "    if not unique_news.empty:\n",
    "        publish_news(api_key, unique_news.to_dict(orient=\"records\"))\n",
    "    else:\n",
    "        print(\"⚠️ No unique news to publish.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b427464-d740-4861-b0d6-1f9559f7a536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 3 news to news_data\\2025-09\\20250928_17.csv\n",
      "📊 Found 3 unique news out of 3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'publish_news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m news_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(news_data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m news_data:\n\u001b[1;32m--> 112\u001b[0m     filter_and_publish(news_data, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstmp_...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ No news items returned.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 103\u001b[0m, in \u001b[0;36mfilter_and_publish\u001b[1;34m(news_data, api_key, folder)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_news)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique news out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(current_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unique_news\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m--> 103\u001b[0m     publish_news(api_key, unique_news\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ No unique news to publish.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'publish_news' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example integration after you parse your API response\n",
    "try:\n",
    "    with open(\"Grok/key.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    grok_api_key = config[\"grok_api_key\"]\n",
    "    convex_api_key = config[\"convex_api_key\"]\n",
    "    \n",
    "    news_data = fetch_security_news(grok_api_key)\n",
    "    \n",
    "    if isinstance(news_data, list) and news_data:\n",
    "        filter_and_publish(news_data, api_key=convex_api_key)\n",
    "    else:\n",
    "        print(\"⚠️ No news items returned.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"❌ Could not parse response content as JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4f0dd-4019-4be3-a94e-76dd75c758f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d7477-b757-4b16-b09e-5e7fac21b4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d27883-4aba-4193-828a-18cdbdee8017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully published news!\n",
      "{'success': True, 'processed': 2, 'successful': 2, 'failed': 0, 'results': [{'index': 1, 'id': 'k17959fe0e8kn25k21j8gszmy57recqg', 'success': True}, {'index': 2, 'id': 'k174b5d94ysg5wdsgvwck084w97rewwn', 'success': True}], 'message': 'Processed 2 threats: 2 successful, 0 failed'}\n"
     ]
    }
   ],
   "source": [
    "api_key = \"stmp_owv8v4cd4rfl9uqqqc5hp\"\n",
    "\n",
    "news_items = [\n",
    "    {\n",
    "        \"title\": \"Food Security Crisis - Northeast Region\",\n",
    "        \"description\": \"Acute malnutrition affecting ~1.8 million children under 5 in Borno State due to US funding cuts. Displaced persons in camps increasingly vulnerable.\",\n",
    "        \"state\": \"Borno\",\n",
    "        \"lga\": \"Dikwa\",\n",
    "        \"status\": \"High\",\n",
    "        \"lat\": 11.1,\n",
    "        \"lng\": 13.2,\n",
    "        \"incidentDate\": \"2025-09-28\",\n",
    "        \"incidentTime\": \"09:00\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Healthcare Worker Violence\",\n",
    "        \"description\": \"Female healthcare worker assaulted at Specialist Hospital Damaturu, Yobe State. Part of wider pattern of workplace violence.\",\n",
    "        \"state\": \"Yobe\",\n",
    "        \"lga\": \"Damaturu\",\n",
    "        \"status\": \"Medium\",\n",
    "        \"incidentDate\": \"2025-09-27\",\n",
    "        \"incidentTime\": \"09:00\"\n",
    "    }\n",
    "]\n",
    "\n",
    "result = publish_news(api_key, news_items)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6991a1f-a750-49e9-b5e7-2f4116d6ff67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e49f95-5ec8-4e58-a392-f2e25449f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes and returns the full text of a news article from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): URL of the news article.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated text from all <p> tags if successful.\n",
    "        None: If the request fails or content is unavailable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            full_text = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "            return full_text\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None  # Return None if the request fails\n",
    "\n",
    "    return None\n",
    "    \n",
    "def connect_to_api_csv(**context):\n",
    "    \"\"\"\n",
    "    Connects to the news API, retrieves news articles for a given day,\n",
    "    processes the articles, and saves them as a CSV file.\n",
    "\n",
    "    The function:\n",
    "    - Formats the execution date.\n",
    "    - Removes an existing file if it exists.\n",
    "    - Creates necessary directories.\n",
    "    - Builds the API URL and retrieves data.\n",
    "    - Scrapes full article content (or uses description if not available).\n",
    "    - Saves the data in CSV format.\n",
    "    \"\"\"\n",
    "    data_path = \"news_data/\"\n",
    "    exec_datetime = datetime.now()\n",
    "    exec_date = exec_datetime.strftime(\"%Y-%m-%d\")\n",
    "    exec_month = exec_datetime.strftime(\"%Y-%m\")\n",
    "\n",
    "    file_name = str(exec_date) + '_news_file.txt'\n",
    "\n",
    "    # Drop file if it exists\n",
    "    if os.path.exists(f\"{data_path}{exec_month}/{file_name}\"):\n",
    "        os.remove(f\"{data_path}{exec_month}/{file_name}\")\n",
    "\n",
    "    # Create required directory for the month\n",
    "    pathlib.Path(f\"{data_path}{exec_month}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build the news API URL\n",
    "    url = (f\"https://newsapi.org/v2/everything?q=(nigeria AND (security OR terrorism OR insurgency OR banditry OR kidnapping OR conflict OR military OR police))&from=2025-09-20&sortBy=publishedAt&language=en&apiKey=11a5a1f11b614295a2c034acd5e2b046\")\n",
    "    response = requests.get(url)\n",
    "    resp_dict = response.json()\n",
    "    articles = resp_dict['articles']\n",
    "\n",
    "    empty_json = {'date': [], 'content': []}\n",
    "\n",
    "    # Process each article in the response\n",
    "    for article in articles:\n",
    "        published_date = article['publishedAt']\n",
    "        published_date = re.findall('[\\d-]+', published_date)[0]\n",
    "        content_url = article['url']\n",
    "        content = get_full_article(content_url)\n",
    "\n",
    "        # If full content is unavailable, use the description\n",
    "        if content is None:\n",
    "            content_descrp = article['description']\n",
    "            empty_json['content'].append(content_descrp)\n",
    "        else:\n",
    "            empty_json['content'].append(content)\n",
    "        empty_json['date'].append(published_date)\n",
    "            \n",
    "    news_df = pd.DataFrame(empty_json)\n",
    "    news_df_shape = news_df.shape\n",
    "    if news_df_shape[0] == 0:\n",
    "        new_row = [exec_date, \"No data for this day\"]\n",
    "        news_df.loc[len(news_df)] = new_row\n",
    "        news_df.to_csv(f\"{data_path}{exec_month}/{file_name}\")\n",
    "    else:\n",
    "        news_df.to_csv(f\"{data_path}{exec_month}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "704bd8ef-3185-4505-8305-35665734475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_to_api_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a299f07a-1b76-4007-837a-a7ec6bce1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('news_data/2025-09/2025-09-24_news_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14387de6-be06-4b58-a91c-f0fd72df948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>The Reds aim to maintain their winning run as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>As a recent project to connect thousands of ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>First Lady of Nigeria, Oluremi Tinubu\\nFirst L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>\\nPLOS Neglected Tropical Diseases is the top ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>Germany’s Autentic Distribution is to shop 80 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>The World Health Organization estimates that a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>The schedule for the second leg of the first p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>Large, selective colleges enroll the greatest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>“If you want a state, kill some Jews,” grumble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>Business Insider Edition \\nThis as-told-to ess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>\\n  Discover a faster, simpler path to publish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>Dishonest governance is rarely a single act or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>When a 41-year-old Nigerian mother of two appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>If a theoretical economy relies on the exchang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>Suspects in the attempted murder of Nigerian s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2025-09-21</td>\n",
       "      <td>\\n            Weeds cover the grave of Yagana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2025-09-21</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\tBy \\n\\n\\tGillian Brockell\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2025-09-20</td>\n",
       "      <td>Skip to comments.\\nPosted on 09/20/2025 11:54:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2025-09-20</td>\n",
       "      <td>“The Ig Nobel awards are arguably the highligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2025-09-20</td>\n",
       "      <td>Potential matchups for the second preliminary ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0        date                                            content\n",
       "0            0  2025-09-23  The Reds aim to maintain their winning run as ...\n",
       "1            1  2025-09-23  As a recent project to connect thousands of ho...\n",
       "2            2  2025-09-23  First Lady of Nigeria, Oluremi Tinubu\\nFirst L...\n",
       "3            3  2025-09-23  \\nPLOS Neglected Tropical Diseases is the top ...\n",
       "4            4  2025-09-23  Germany’s Autentic Distribution is to shop 80 ...\n",
       "5            5  2025-09-23  The World Health Organization estimates that a...\n",
       "6            6  2025-09-23  The schedule for the second leg of the first p...\n",
       "7            7  2025-09-23  Large, selective colleges enroll the greatest ...\n",
       "8            8  2025-09-22  “If you want a state, kill some Jews,” grumble...\n",
       "9            9  2025-09-22  Business Insider Edition \\nThis as-told-to ess...\n",
       "10          10  2025-09-22  \\n  Discover a faster, simpler path to publish...\n",
       "11          11  2025-09-22  Dishonest governance is rarely a single act or...\n",
       "12          12  2025-09-22  When a 41-year-old Nigerian mother of two appe...\n",
       "13          13  2025-09-22  If a theoretical economy relies on the exchang...\n",
       "14          14  2025-09-22  Suspects in the attempted murder of Nigerian s...\n",
       "15          15  2025-09-21  \\n            Weeds cover the grave of Yagana ...\n",
       "16          16  2025-09-21  \\n\\t\\t\\t\\t\\t\\t\\tBy \\n\\n\\tGillian Brockell\\n\\n\\...\n",
       "17          17  2025-09-20  Skip to comments.\\nPosted on 09/20/2025 11:54:...\n",
       "18          18  2025-09-20  “The Ig Nobel awards are arguably the highligh...\n",
       "19          19  2025-09-20  Potential matchups for the second preliminary ..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc907a2-9a48-40df-b186-089fc17981a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Udoh\\AppData\\Local\\Temp\\ipykernel_13412\\3310649270.py:43: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow()  # use UTC for consistency\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[Request ID: 275c2a8db5ce375b] Server Error\nUncaught Error: Not authenticated\n    at handler (../convex/threats.ts:70:4)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     43\u001b[0m         now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mutcnow()  \u001b[38;5;66;03m# use UTC for consistency\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m         client\u001b[38;5;241m.\u001b[39mmutation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreats:addThreat\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlng\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlga\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: state,\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;66;03m# auto-filled timestamps\u001b[39;00m\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincidentDate\u001b[39m\u001b[38;5;124m\"\u001b[39m: now\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincidentTime\u001b[39m\u001b[38;5;124m\"\u001b[39m: now\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     55\u001b[0m         })\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ News inserted into Convex threats table with timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\convex\\__init__.py:216\u001b[0m, in \u001b[0;36mConvexClient.mutation\u001b[1;34m(self, name, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmutation\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, args: FunctionArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform the mutation `name` with `args` returning the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmutation(name, coerce_args_to_convex(args))\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvexerror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConvexError(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mException\u001b[0m: [Request ID: 275c2a8db5ce375b] Server Error\nUncaught Error: Not authenticated\n    at handler (../convex/threats.ts:70:4)\n"
     ]
    }
   ],
   "source": [
    "from convex import ConvexClient\n",
    "from datetime import datetime\n",
    "\n",
    "key = \"eyJ2MiI6IjAzZWM2YjgzMGU1MDQyOTBhMDM2ZTVhMzRiMGY0N2MzIn0=\"\n",
    "\n",
    "# Connect to your Convex deployment\n",
    "client = ConvexClient(\"https://fantastic-mammoth-699.convex.cloud\")\n",
    "#client.set_auth(key) \n",
    "\n",
    "news_items = [\n",
    "    {\n",
    "        \"title\": \"Food Security Crisis - Northeast Region\",\n",
    "        \"description\": (\n",
    "            \"Acute malnutrition affecting ~1.8 million children under 5. \"\n",
    "            \"US funding cuts have impacted displaced populations in camps, \"\n",
    "            \"including Fulatari camp in Dikwa, Borno State. \"\n",
    "            \"Increased vulnerability of IDPs fleeing Boko Haram terrorism.\"\n",
    "        ),\n",
    "        \"states\": [\"Borno\"]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Healthcare Worker Violence\",\n",
    "        \"description\": (\n",
    "            \"Physical assault on female healthcare worker at Specialist Hospital Damaturu. \"\n",
    "            \"Part of broader pattern of violence against healthcare workers. \"\n",
    "            \"Surveys show 64% of Kaduna and 88% of Abia health workers experience workplace violence.\"\n",
    "        ),\n",
    "        \"states\": [\"Yobe\", \"Kaduna\", \"Abia\"]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Criminal Activity\",\n",
    "        \"description\": (\n",
    "            \"Suspects arrested in Kano for attempted murder of Nigerian singer Lil Kesh. \"\n",
    "            \"Involved theft of ~₦2 million in Lagos.\"\n",
    "        ),\n",
    "        \"states\": [\"Kano\", \"Lagos\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Insert into Convex threats table\n",
    "for item in news_items:\n",
    "    for state in item[\"states\"]:\n",
    "        now = datetime.utcnow()  # use UTC for consistency\n",
    "        client.mutation(\"threats:addThreat\", {\n",
    "            \"description\": item[\"description\"],\n",
    "            \"lat\": 0.0,\n",
    "            \"lng\": 0.0,\n",
    "            \"lga\": \"unknown\",\n",
    "            \"state\": state,\n",
    "            \"status\": \"High\",\n",
    "            \"title\": item[\"title\"],\n",
    "            # auto-filled timestamps\n",
    "            \"incidentDate\": now.strftime(\"%Y-%m-%d\"),\n",
    "            \"incidentTime\": now.strftime(\"%H:%M:%S\"),\n",
    "        })\n",
    "\n",
    "print(\"✅ News inserted into Convex threats table with timestamps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200417af-afed-4c4c-91d3-f5d3f762f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Airflow Modules\n",
    "import airflow\n",
    "from airflow.models import Variable\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.utils.email import send_email\n",
    "\n",
    "# Trad NLP Modules\n",
    "from joblib import load\n",
    "import nltk\n",
    "# Download necessary NLTK resources quietly\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import cloudpickle\n",
    "\n",
    "# Useful Functionality Modules\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "import shutil\n",
    "import requests\n",
    "import re\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "# Google Gemini Modules\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Retrieve credentials from Airflow Variables\n",
    "api_key = Variable.get(\"NEWS_API_KEY\")\n",
    "airflow_vars = Variable.get(\"email_vars\", deserialize_json=True)\n",
    "#airflow_vars_str = Variable.get(\"email_vars\", deserialize_json=True)\n",
    "#airflow_vars = json.loads(str(airflow_vars_str))\n",
    "gemini_api_key = Variable.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "SMTP_URL = airflow_vars['smtp_url']\n",
    "SMTP_USER = airflow_vars['smtp_user']\n",
    "SMTP_PASSWORD = airflow_vars['smtp_password']\n",
    "\n",
    "# Default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': dt.datetime(2025, 4, 1),\n",
    "}\n",
    "\n",
    "# Defining path variables\n",
    "parent_path = \"/app/News-Api-Project/\"\n",
    "data_path = f\"{parent_path}data/\"\n",
    "labeled_data_path = f\"{parent_path}labeled_data/\"\n",
    "model_path = f\"{parent_path}models/\"\n",
    "compressed_data_path = f\"{parent_path}compressed_data/\"\n",
    "ai_content = f\"{parent_path}ai_analysis/\"\n",
    "\n",
    "# DBT file paths\n",
    "stock_news_path = \"/app/News-Api-Project/My_DBT/Airflow_Stock_Sentiment_Project/models/airflow_stock_sentiment_models/Stock_News.sql\"\n",
    "non_stock_news_path = \"/app/News-Api-Project/My_DBT/Airflow_Stock_Sentiment_Project/models/airflow_stock_sentiment_models/Non_Stock_News.sql\"\n",
    "dbt_path = \"/app/News-Api-Project/My_DBT/Airflow_Stock_Sentiment_Project\"\n",
    "\n",
    "# DBT file content for stock news and non-stock news\n",
    "stock_news_dbt_file_content = \"\"\"{{ config(materialized='incremental',\n",
    "unique_key='content') }}\n",
    "\n",
    "select * from News_DB.Full_News_Table \n",
    "\n",
    "{% if is_incremental() %}\n",
    "    where label =1 and date={{ ds }}\n",
    "{% endif %}\"\"\"\n",
    "\n",
    "non_stock_news_dbt_file_content = \"\"\"{{ config(materialized='incremental',\n",
    "unique_key='content') }}\n",
    "\n",
    "select * from News_DB.Full_News_Table \n",
    "\n",
    "{% if is_incremental() %}\n",
    "    where label =1 and date={{ ds }}\n",
    "{% endif %}\"\"\"\n",
    "\n",
    "\n",
    "def get_full_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes and returns the full text of a news article from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): URL of the news article.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated text from all <p> tags if successful.\n",
    "        None: If the request fails or content is unavailable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            full_text = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "            return full_text\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None  # Return None if the request fails\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def connect_to_api_csv(**context):\n",
    "    \"\"\"\n",
    "    Connects to the news API, retrieves news articles for a given day,\n",
    "    processes the articles, and saves them as a CSV file.\n",
    "\n",
    "    The function:\n",
    "    - Formats the execution date.\n",
    "    - Removes an existing file if it exists.\n",
    "    - Creates necessary directories.\n",
    "    - Builds the API URL and retrieves data.\n",
    "    - Scrapes full article content (or uses description if not available).\n",
    "    - Saves the data in CSV format.\n",
    "    \"\"\"\n",
    "    exec_datetime = context[\"execution_date\"]\n",
    "    exec_date = exec_datetime.strftime(\"%Y-%m-%d\")\n",
    "    exec_month = exec_datetime.strftime(\"%Y-%m\")\n",
    "\n",
    "    file_name = str(exec_date) + '_news_file.txt'\n",
    "\n",
    "    # Drop file if it exists\n",
    "    if os.path.exists(f\"{data_path}{exec_month}/{file_name}\"):\n",
    "        os.remove(f\"{data_path}{exec_month}/{file_name}\")\n",
    "\n",
    "    # Create required directory for the month\n",
    "    pathlib.Path(f\"{data_path}{exec_month}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build the news API URL\n",
    "    url = (f\"https://newsapi.org/v2/everything?from={exec_date}&to={exec_date}&language=en&q=(market OR stock)&apiKey={api_key}\")\n",
    "    response = requests.get(url)\n",
    "    resp_dict = response.json()\n",
    "    articles = resp_dict['articles']\n",
    "\n",
    "    empty_json = {'date': [], 'content': []}\n",
    "\n",
    "    # Process each article in the response\n",
    "    for article in articles:\n",
    "        published_date = article['publishedAt']\n",
    "        published_date = re.findall('[\\d-]+', published_date)[0]\n",
    "        content_url = article['url']\n",
    "        content = get_full_article(content_url)\n",
    "\n",
    "        # If full content is unavailable, use the description\n",
    "        if content is None:\n",
    "            content_descrp = article['description']\n",
    "            empty_json['content'].append(content_descrp)\n",
    "        else:\n",
    "            empty_json['content'].append(content)\n",
    "        empty_json['date'].append(published_date)\n",
    "            \n",
    "    news_df = pd.DataFrame(empty_json)\n",
    "    news_df_shape = news_df.shape\n",
    "    if news_df_shape[0] == 0:\n",
    "        new_row = [exec_date, \"No data for this day\"]\n",
    "        news_df.loc[len(news_df)] = new_row\n",
    "        news_df.to_csv(f\"{data_path}{exec_month}/{file_name}\")\n",
    "    else:\n",
    "        news_df.to_csv(f\"{data_path}{exec_month}/{file_name}\")\n",
    "\n",
    "\n",
    "# Load pre-trained TF-IDF vectorizer using cloudpickle\n",
    "with open(f\"{model_path}/tfidf_vectorizer_300.pkl\", \"rb\") as f:\n",
    "    tfidf_loaded = cloudpickle.load(f)\n",
    "\n",
    "\n",
    "def filter_news(**context):\n",
    "    \"\"\"\n",
    "    Filters news data to extract stock-related articles using a pre-trained model.\n",
    "\n",
    "    The function:\n",
    "    - Loads the daily news CSV file.\n",
    "    - Cleans the data by dropping unnecessary columns and rows with missing values.\n",
    "    - Transforms the content using a loaded TF-IDF vectorizer.\n",
    "    - Predicts labels using a pre-trained model.\n",
    "    - Saves the labeled data as a line-delimited JSON (.jsonl) file.\n",
    "    \"\"\"\n",
    "    exec_datetime = context[\"execution_date\"]    # <datetime> with timezone\n",
    "    exec_date = exec_datetime.strftime(\"%Y-%m-%d\")\n",
    "    exec_month = exec_datetime.strftime(\"%Y-%m\")\n",
    "\n",
    "    file_name = str(exec_date) + '_news_file.txt'\n",
    "\n",
    "    # Create required directories for labeled data and models\n",
    "    pathlib.Path(f\"{labeled_data_path}{exec_month}\").mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load and clean the dataset\n",
    "    df = pd.read_csv(f\"{data_path}{exec_month}/{file_name}\")\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Transform content using the loaded TF-IDF vectorizer\n",
    "    char_array = tfidf_loaded.transform(df.content).toarray()\n",
    "    frequency_matrix = pd.DataFrame(char_array, columns=tfidf_loaded.get_feature_names_out())\n",
    "\n",
    "    # Load the model and predict labels\n",
    "    model = load(f\"{model_path}/lgb.joblib\")\n",
    "    model_pred = model.predict(frequency_matrix)\n",
    "    df['label'] = model_pred\n",
    "    df[\"content\"] = df[\"content\"].str.replace(\"\\n\", \" \", regex=True)\n",
    "\n",
    "    # Save the labeled data as line-delimited JSON (.jsonl)\n",
    "    df.to_json(f\"{labeled_data_path}{exec_month}/\" + file_name.replace('.txt', '_labeled.jsonl'), orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def compress_choice(**context):\n",
    "    \"\"\"\n",
    "    Determines whether to compress and remove files based on the next execution day.\n",
    "\n",
    "    Returns:\n",
    "        str: 'compress_and_remove_files' if next execution day is the 1st,\n",
    "             otherwise 'do_nothing_start_dbt'.\n",
    "    \"\"\"\n",
    "    exec_datetime = context[\"next_execution_date\"]\n",
    "    exec_day = exec_datetime.day  # Get day of month\n",
    "    if exec_day == 1:\n",
    "        return 'compress_and_remove_files'\n",
    "    else:\n",
    "        return 'do_nothing_start_dbt'\n",
    "\n",
    "\n",
    "def compress_and_remove_files_(**context):\n",
    "    \"\"\"\n",
    "    Compresses raw news data for the month and removes the original folder if it's time to archive.\n",
    "\n",
    "    The function:\n",
    "    - Checks if it's the start of a new month.\n",
    "    - Compresses the folder containing raw news data.\n",
    "    - Removes the original uncompressed folder after archiving.\n",
    "    \"\"\"\n",
    "    next_exec_datetime = context[\"next_execution_date\"]\n",
    "    next_exec_day = next_exec_datetime.day  # Get day of month\n",
    "\n",
    "    exec_datetime = context[\"execution_date\"]\n",
    "    exec_month = exec_datetime.strftime(\"%Y-%m\")\n",
    "\n",
    "    compressed_file_name = f\"{exec_month}_raw_news_data\"\n",
    "\n",
    "    # Create compressed data directory if it doesn't exist\n",
    "    if os.path.exists(f\"{compressed_data_path}\"):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(f\"{compressed_data_path}\")\n",
    "\n",
    "    # If it's the 1st day and the archive does not exist, then compress\n",
    "    if next_exec_day == 1 and os.path.exists(f\"{compressed_data_path}{compressed_file_name}.zip\"):\n",
    "        pass\n",
    "    elif next_exec_day == 1:\n",
    "        # Path to the folder to compress\n",
    "        folder_to_compress = f\"{data_path}{exec_month}\"\n",
    "\n",
    "        # Output archive path (without .zip extension)\n",
    "        archive_name = f\"{compressed_data_path}{compressed_file_name}\"\n",
    "\n",
    "        # Create a zip archive of the folder\n",
    "        shutil.make_archive(archive_name, 'zip', folder_to_compress)\n",
    "\n",
    "        # Remove the original folder after compression\n",
    "        shutil.rmtree(f\"{data_path}{exec_month}\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def LLM_advice(**context):\n",
    "    \"\"\"\n",
    "    Generates AI-based stock sentiment advice using Google Gemini based on extracted news.\n",
    "\n",
    "    The function:\n",
    "    - Retrieves news content from XCom.\n",
    "    - Sends the news content to Google Gemini for summarization and stock advice.\n",
    "    - Saves the AI-generated advice to a text file.\n",
    "    \"\"\"\n",
    "    exec_datetime = context[\"execution_date\"]\n",
    "    exec_date = exec_datetime.strftime(\"%Y-%m-%d\")\n",
    "    exec_month = exec_datetime.strftime(\"%Y-%m\")\n",
    "\n",
    "    llm_output = f\"{ai_content}{exec_month}/{exec_date}_llm_advice.txt\"\n",
    "\n",
    "    # System instruction for the language model\n",
    "    sys_instruct = \"You are a Stock Sentiment Analyst. Your work is to summarize news data and give stock investment advice in a nicely formated way.\"\n",
    "    client = genai.Client(api_key=gemini_api_key)\n",
    "    news = context[\"task_instance\"].xcom_pull(\n",
    "        task_ids=\"extract_stock_news_info\", key=\"return_value\"\n",
    "    )\n",
    "    news_as_text = ''\n",
    "    for ind, cont in enumerate(news):\n",
    "        news_as_text += f\"News_{ind+1} \\n '\\n'.join{[str(a) for a in cont]} \\n\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=sys_instruct),\n",
    "        contents=[\"\"\"These are the recent stock news for today. Please in a summarized way, tell me 1. The stocks mentioned today 2. Which of these stocks had a bad sentiment 3. Which had good sentiment 4. Which would you advice me to keep an eye on \\n\"\"\" + news_as_text]\n",
    "    )\n",
    "    # Create directory for AI content if it doesn't exist\n",
    "    if os.path.exists(f\"{ai_content}{exec_month}\"):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(f\"{ai_content}{exec_month}\")\n",
    "    # Save the AI-generated advice to file\n",
    "    with open(llm_output, \"w\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "\n",
    "def format_file_to_html(file_path):\n",
    "    \"\"\"\n",
    "    Converts a Markdown-formatted text file into a styled HTML email.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the Markdown file.\n",
    "\n",
    "    Returns:\n",
    "        str: HTML content with inline CSS styling.\n",
    "    \"\"\"\n",
    "    # Read the raw Markdown text from the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "    \n",
    "    # Convert Markdown to HTML\n",
    "    html_body = markdown.markdown(raw_text)\n",
    "    \n",
    "    # Wrap the HTML body in a complete HTML email template with inline CSS\n",
    "    html_email = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "      <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>AI Generated Stock News & Investment Advice</title>\n",
    "        <style>\n",
    "          body {{\n",
    "            font-family: Arial, sans-serif;\n",
    "            line-height: 1.6;\n",
    "            color: #333;\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            background-color: #f9f9f9;\n",
    "          }}\n",
    "          .container {{\n",
    "            max-width: 800px;\n",
    "            margin: 30px auto;\n",
    "            background-color: #fff;\n",
    "            padding: 20px;\n",
    "            border: 1px solid #ddd;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "          }}\n",
    "          h1 {{\n",
    "            color: #00539C;\n",
    "          }}\n",
    "          h2 {{\n",
    "            color: #0073e6;\n",
    "            border-bottom: 1px solid #ddd;\n",
    "            padding-bottom: 5px;\n",
    "          }}\n",
    "          ul {{\n",
    "            list-style-type: disc;\n",
    "            margin-left: 20px;\n",
    "          }}\n",
    "          .good {{\n",
    "            color: green;\n",
    "            font-weight: bold;\n",
    "          }}\n",
    "          .bad {{\n",
    "            color: red;\n",
    "            font-weight: bold;\n",
    "          }}\n",
    "          .disclaimer {{\n",
    "            font-size: 0.9em;\n",
    "            color: #666;\n",
    "            margin-top: 20px;\n",
    "            border-top: 1px solid #ccc;\n",
    "            padding-top: 10px;\n",
    "          }}\n",
    "        </style>\n",
    "      </head>\n",
    "      <body>\n",
    "        <div class=\"container\">\n",
    "          <h2>AI Analysis on Stock News Provided</h2>\n",
    "          {html_body}\n",
    "        </div>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_email\n",
    "\n",
    "\n",
    "def notify_email(**context):\n",
    "    \"\"\"\n",
    "    Formats the AI advice file as an HTML email and sends it to designated recipients.\n",
    "\n",
    "    The function:\n",
    "    - Converts the advice file from Markdown to styled HTML.\n",
    "    - Composes the email with subject and attachments.\n",
    "    - Sends the email using Airflow's send_email utility.\n",
    "    \"\"\"\n",
    "    exec_datetime = context[\"execution_date\"]\n",
    "    exec_date = exec_datetime.strftime(\"%Y-%m-%d\")\n",
    "    exec_month = exec_datetime.strftime(\"%Y-%m\")\n",
    "\n",
    "    file_path = f\"{ai_content}{exec_month}/{exec_date}_llm_advice.txt\"\n",
    "\n",
    "    subject = f\"AI Analysis Report on Stock News Today {exec_date}\"\n",
    "    html_content = format_file_to_html(file_path)\n",
    "    # Specify file attachments for the email\n",
    "    attachments = [file_path]\n",
    "    \n",
    "    send_email(\n",
    "        to=[\"udohchigozie2017@gmail.com\"],\n",
    "        subject=subject,\n",
    "        html_content=html_content,\n",
    "        files=attachments\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the DAG and its tasks using a context manager\n",
    "with DAG(dag_id=\"Stock_sentiment_analysis\", default_args=default_args, \n",
    "         schedule_interval=\"@daily\", catchup=False) as dag:\n",
    "\n",
    "    # Task: Retrieve news data from the API and save as CSV\n",
    "    get_data = PythonOperator(\n",
    "        task_id='get_news_data', \n",
    "        python_callable=connect_to_api_csv\n",
    "    )\n",
    "\n",
    "    # Task: Filter and label news data for stock relevance\n",
    "    filter_data = PythonOperator(\n",
    "        task_id='get_only_stock_news', \n",
    "        python_callable=filter_news\n",
    "    )\n",
    "\n",
    "    # Task: Load filtered data into the database (Vertica)\n",
    "    load_to_table = SQLExecuteQueryOperator(\n",
    "        task_id=\"load_to_vertica\",\n",
    "        conn_id=\"vertica\",\n",
    "        sql=r\"\"\"COPY News_DB.Full_News_Table from LOCAL '{{ params.labeled_data_path }}{{ ds[:7] }}/{{ ds }}_news_file_labeled.jsonl' PARSER fjsonparser();\"\"\",\n",
    "        params={'labeled_data_path': labeled_data_path}\n",
    "    )\n",
    "\n",
    "    # Task: Decide whether to compress raw data files\n",
    "    compress_or_not = BranchPythonOperator(\n",
    "        task_id='compress_choice', \n",
    "        python_callable=compress_choice\n",
    "    )\n",
    "\n",
    "    # Set task dependencies: get data -> filter data -> load data -> branch decision\n",
    "    get_data >> filter_data >> load_to_table >> compress_or_not\n",
    "\n",
    "    # Task: Compress and remove files if the branch condition is met\n",
    "    compress_and_remove_files = PythonOperator(\n",
    "        task_id='compress_and_remove_files', \n",
    "        python_callable=compress_and_remove_files_\n",
    "    )\n",
    "\n",
    "    # Dummy task: No action needed if compression is not required\n",
    "    do_nothing = DummyOperator(task_id='do_nothing_start_dbt', dag=dag)\n",
    "\n",
    "    # Task: Execute DBT commands to separate stock and non-stock news and run DBT models\n",
    "    start_dbt = BashOperator(\n",
    "        task_id=\"separate_stock_news\",\n",
    "        bash_command=\"\"\"\n",
    "        rm {{ params.stock_path }} {{ params.non_stock_path }} &&\n",
    "        \n",
    "        echo \"{% raw %}{{ config(materialized='incremental', unique_key='content') }} \n",
    "        select * from News_DB.Full_News_Table \n",
    "        {% if is_incremental() %}\n",
    "            where label = {% endraw %}{{ params.one }}{% raw %} and date = '{% endraw %}{{ ds }}{% raw %}'\n",
    "        {% endif %}{% endraw %}\" >> {{ params.stock_path }} &&\n",
    "\n",
    "        echo \"{% raw %}{{ config(materialized='incremental', unique_key='content') }} \n",
    "        select * from News_DB.Full_News_Table \n",
    "        {% if is_incremental() %}\n",
    "            where label = {% endraw %}{{ params.zero }}{% raw %} and date = '{% endraw %}{{ ds }}{% raw %}'\n",
    "        {% endif %}{% endraw %}\" >> {{ params.non_stock_path }} &&\n",
    "\n",
    "        cd {{ params.dbt_path }} && dbt run\n",
    "    \"\"\",\n",
    "        params={\n",
    "            \"stock_path\": stock_news_path,\n",
    "            \"non_stock_path\": non_stock_news_path,\n",
    "            \"dbt_path\": dbt_path,\n",
    "            \"one\": 1,\n",
    "            \"zero\": 0\n",
    "        },\n",
    "        trigger_rule=\"none_failed\"\n",
    "    )\n",
    "\n",
    "    # Define branch dependencies: compress or not -> start DBT\n",
    "    compress_or_not >> [compress_and_remove_files, do_nothing] >> start_dbt\n",
    "\n",
    "    # Task: Extract stock news content from the database for AI processing\n",
    "    extract_news_info = SQLExecuteQueryOperator(\n",
    "        task_id=\"extract_stock_news_info\",\n",
    "        conn_id=\"vertica\",\n",
    "        sql=r\"\"\"select content from News_DB.Stock_News where date='{{ ds }}' and label=1;\"\"\",\n",
    "        do_xcom_push=True\n",
    "    )\n",
    "\n",
    "    # Task: Generate AI-based stock recommendations using LLM\n",
    "    get_ai_recommendation = PythonOperator(\n",
    "        task_id='LLM_advice', \n",
    "        python_callable=LLM_advice\n",
    "    )\n",
    "\n",
    "    # Set dependency: after DBT, extract news info then generate AI recommendations\n",
    "    start_dbt >> extract_news_info >> get_ai_recommendation\n",
    "\n",
    "    # Task: Send the AI-generated stock insights via email\n",
    "    notify = PythonOperator(\n",
    "        task_id=\"send_ai_stock_insight\", \n",
    "        python_callable=notify_email\n",
    "    )\n",
    "    \n",
    "    # Set final dependency: after AI recommendations, send notification email\n",
    "    get_ai_recommendation >> notify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
